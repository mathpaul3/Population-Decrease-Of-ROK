## 대한민국의 인구는 언제 1명이 될까?

**경사하강법(Gradient Descent)과 수열의 극한 사용**



현재 대한민국은 저출산 고령화 문제를 겪고 있다.

노령 인구의 증가로 인해 사망률은 점차 높아지는 데 비해 출산율이 급격히 낮아지고 있기 때문이다.

그런데 문득, 이대로 가다간 한국의 인구가 언젠가 1명이 되지는 않을까? 하는 생각이 들었다.

그래서 직접 한 번 구해보기로 했다.



먼저, [통계청](http://kostat.go.kr/portal/korea/index.action)에서 출생아와 사망자의 인구동태를 찾아보았다.
![KakaoTalk_20200822_133828572_02](./KakaoTalk_20200822_133828572_02.jpg)

도표를 보자마자 난관에 봉착했다.

출생아수가 거의 요동치다시피하며 감소하고 있다.

이러면 등차 혹은 등비수열이 아니기 때문에 수열의 극한을 사용할 수가 없다.

그렇다면 아래 그림과 같이 출생아수와 사망자수를 대표하는 직선을 찾아야하는데,

![KakaoTalk_20200822_133828572](./KakaoTalk_20200822_133828572.jpg)

1970년의 값과 2018년의 값을 가지고 냅다 직선을 그어버리는 것은 각각의 값들을 대표한다고 보기 어렵다.

이 때 인공지능을 공부하며 익힌 경사하강법<sub>Gradient Descent</sub>이 떠올라, 이를 통해 각 데이터를 대표하는 직선을

구해보기로 했다.





먼저, 출생아수를 대표하는 가설함수<sub>Hypothesis</sub> H(x)가 있다고 하자.

x값은 년도, H(x)값은 출생아수 이다.

이 함수는 일차함수이기 때문에 H(x) = Wx+b 와 같이 나타낼 수 있다.

(보통 인공지능 신경망에서 W는 가중치<sub>Weight</sub>를, b는 편향<sub>bias</sub>을 나타내지만, 지금은 직선의 기울기와 y절편이다.)

이 때, 임의의 W와 b 값에 대한 평균제곱오차<sub>MSE, Mean Squared Error</sub>(혹은 비용함수<sub>cost function</sub>)를 구한다.
$$
MSE = \frac{1}{n}\sum ^{n}_{i=1}({y_i - \hat{y_i}})^2\\\;\\
※ n은\; 년도의\; 개수이고,\; y_i는\; i번째\; 년도의\; 출생아수,\; \hat{y_i}는\; 가설함수의\; i번째\; 년도의\; 출생아수이다.
$$


MSE를 정리해보면 다음과 같은 식이 나온다.
$$
\begin{align}
MSE&=\frac{1}{n}\sum ^n _{i=1}({y_i}-\hat{y_i})^2\\
&=\frac{1}{n}\{(y_1-wx_1-b)^2+\dots+(y_n-wx_n-b)^2\}\\
&=\frac{1}{n}\{(x_1^2+\dots+x_n^2)w^2+★\}\qquad(for\;w)\\
&=\frac{1}{n}\{nb^2+◆\}\qquad(for\;b)\\\;\\
&※편의상\; 불필요한\; 값들은\; ★과\; ◆로\; 대체함.
\end{align}
$$


![img](https://t1.daumcdn.net/cfile/tistory/990C103A5C764DDE25)

위에서 도출한 식에 따르면 W와 b에 따른 비용함수<sub>MSE</sub>는 위 그림처럼 이차함수로 나타낼 수 있다.



여기서, MSE는 오차들의 합이기 때문에 MSE가 최소가 되는 W와 b의 값이 최적의 직선을 나타낼 것이다.

<img src="https://t1.daumcdn.net/cfile/tistory/997774505C7738DC02" alt="img" />

 그렇다면 이 값들은 어떻게 찾을까?



임의의 W값이 아래 그림과 같이 주어졌다고 하자.

<img src="https://t1.daumcdn.net/cfile/tistory/99FF5C3B5C773F760B" alt="img" style="zoom:50%;" align="left"/>

최적의 W값을 구하려면

초기 W값에서, 비용함수<sub>MSE</sub>의 변수 W에 대한 편미분에 학습률(η)이라 불리는 양의 실수를 곱한 값을 빼나간다.
$$
W := W-η\frac{∂}{∂W}MSE
$$
이러면 비용함수가 감소하는 구간에서는 편미분 값이 음수, η값이 양수이므로 조금씩 오른쪽으로 이동하고,

비용함수가 증가하는 구간에서는 편미분 값이 양수, η값이 양수이므로 조금씩 왼쪽으로 이동한다.

(여기서 학습률(η)이 너무 작으면 최적의 W값을 구하는데 시간이 너무 오래 걸리고,

학습률(η)이 너무 크면 최적의 W값을 건너뛰고 발산해버릴 수도 있으므로, 적당히 0.001정도로 설정한다)

(이 방식이 경사하강법인 이유가 여기서 나오는데, 편미분 값(접선의 경사)을 점점 하강시키기 때문에 이러한 이름이 붙었다)

점점 하강시켜 기울기가 0이되면 최적의 W값을 찾을 수 있다.

같은 방식으로 최적의 b값도 찾으면, 출생아수를 대표하는 가설함수 H(x)를 구할 수 있게 된다.

사망자수를 대표하는 가설함수 역시 같은 방식으로 구한다.



그럼, 코드를 한 번 짜보도록 하자.

![code_result](./code_result.png)
![code_result_d](./code_result_d.png)

출생아수 함수는 H(x) = -19331.236x + 39200083.019 이고,

사망자수 함수는 H(x) = 2378.246x - 4492000.047 이었다.

$$
\begin{align}
&출생아수\; 함수는\; H(x) = -19331.236x + 39200083.019\; 이고,\\
&사망자수\; 함수는\; H(x) = 2378.246x - 4492000.047\; 이었다.
\end{align}
$$

